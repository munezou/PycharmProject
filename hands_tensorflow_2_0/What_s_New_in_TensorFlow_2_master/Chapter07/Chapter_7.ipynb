{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunk = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n",
    "head1 = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n",
    "head2 = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n",
    "\n",
    "path1 = tf.keras.Sequential([trunk, head1])\n",
    "path2 = tf.keras.Sequential([trunk, head2])\n",
    "\n",
    "# Train on primary dataset\n",
    "for x, y in main_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    prediction = path1(x)\n",
    "    loss = loss_fn_head1(prediction, y)\n",
    "  # Simultaneously optimize trunk and head1 weights.\n",
    "  gradients = tape.gradient(loss, path1.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, path1.trainable_variables))\n",
    "\n",
    "# Fine-tune second head, reusing the trunk\n",
    "for x, y in small_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    prediction = path2(x)\n",
    "    loss = loss_fn_head2(prediction, y)\n",
    "  # Only optimize head2 weights, not trunk weights\n",
    "  gradients = tape.gradient(loss, head2.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, head2.trainable_variables))\n",
    "\n",
    "# You can publish just the trunk computation for other people to reuse.\n",
    "tf.saved_model.save(trunk, output_path)\n",
    "\n",
    "@tf.function\n",
    "def train(model, dataset, optimizer):\n",
    "  for x, y in dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "      prediction = model(x)\n",
    "      loss = loss_fn(prediction, y)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "model.fit(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "train_data = tf.ones(shape=(1, 28, 28, 1))\n",
    "test_data = tf.ones(shape=(1, 28, 28, 1))\n",
    "\n",
    "train_out = model(train_data, training=True)\n",
    "\n",
    "test_out = model(test_data, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.ones(shape=(2,2)), name=\"W\")\n",
    "b = tf.Variable(tf.zeros(shape=(2)), name=\"b\")\n",
    "\n",
    "@tf.function\n",
    "def forward(x):\n",
    "  return W * x + b\n",
    "\n",
    "out_a = forward([1,0])\n",
    "out_b = forward([0,1])\n",
    "\n",
    "regularizer = tf.keras.regularizers.l2(0.04)\n",
    "reg_loss = regularizer(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model using Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(CustomLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.w = self.add_weight(\n",
    "        shape=input_shape[1:],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.keras.initializers.ones(),\n",
    "        regularizer=tf.keras.regularizers.l2(0.02),\n",
    "        trainable=True)\n",
    "\n",
    "  # Call method will sometimes get used in graph mode,\n",
    "  # training will get turned into a tensor\n",
    "  @tf.function\n",
    "  def call(self, inputs, training=None):\n",
    "    if training:\n",
    "      return inputs + self.w\n",
    "    else:\n",
    "      return inputs + self.w * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model.fit training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "metrics_names = model.metrics_names\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  #Reset the metric accumulators\n",
    "  model.reset_metrics()\n",
    "\n",
    "  for image_batch, label_batch in train_data:\n",
    "    result = model.train_on_batch(image_batch, label_batch)\n",
    "    print(\"train: \",\n",
    "          \"{}: {:.3f}\".format(metrics_names[0], result[0]),\n",
    "          \"{}: {:.3f}\".format(metrics_names[1], result[1]))\n",
    "  for image_batch, label_batch in test_data:\n",
    "    result = model.test_on_batch(image_batch, label_batch,\n",
    "                                 # return accumulated metrics\n",
    "                                 reset_metrics=False)\n",
    "  print(\"\\neval: \",\n",
    "        \"{}: {:.3f}\".format(metrics_names[0], result[0]),\n",
    "        \"{}: {:.3f}\".format(metrics_names[1], result[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Tape Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inputs, training=True)\n",
    "    regularization_loss = tf.math.add_n(model.losses)\n",
    "    pred_loss = loss_fn(labels, predictions)\n",
    "    total_loss = pred_loss + regularization_loss\n",
    "\n",
    "  gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  for inputs, labels in train_data:\n",
    "    train_step(inputs, labels)\n",
    "  print(\"Finished epoch\", epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loop with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the metrics\n",
    "loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inputs, training=True)\n",
    "    regularization_loss = tf.math.add_n(model.losses)\n",
    "    pred_loss = loss_fn(labels, predictions)\n",
    "    total_loss = pred_loss + regularization_loss\n",
    "\n",
    "  gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  # Update the metrics\n",
    "  loss_metric.update_state(total_loss)\n",
    "  accuracy_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  # Reset the metrics\n",
    "  loss_metric.reset_states()\n",
    "  accuracy_metric.reset_states()\n",
    "\n",
    "  for inputs, labels in train_data:\n",
    "    train_step(inputs, labels)\n",
    "  # Get the metric results\n",
    "  mean_loss = loss_metric.result()\n",
    "  mean_accuracy = accuracy_metric.result()\n",
    "\n",
    "  print('Epoch: ', epoch)\n",
    "  print('  loss:     {:.3f}'.format(mean_loss))\n",
    "  print('  accuracy: {:.3f}'.format(mean_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
